{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extractor - Hybrid LangChain Pipeline\n",
    "\n",
    "**Streamlined document extraction using YAML-configured LangChain v1.0 pipeline.**\n",
    "\n",
    "**Key Features:**\n",
    "- YAML-based configuration (no code changes needed)\n",
    "- Hot-reload capability (edit prompts live)\n",
    "- LangChain v1.0 (BaseChatModel with multi-modal messages)\n",
    "- Easy model switching (Llama ‚Üî InternVL3)\n",
    "- Comprehensive analytics and visualizations\n",
    "- Production-ready validation and cleaning (81.8% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">‚úÖ Hybrid LangChain pipeline loaded successfully!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m‚úÖ Hybrid LangChain pipeline loaded successfully!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üìã Configuration: config/models.yaml, config/prompts.yaml</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müìã Configuration: config/models.yaml, config/prompts.yaml\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'correlation'\n",
    "\n",
    "# Standard library\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "# LangChain Integration\n",
    "from common.langchain_chains import create_pipeline\n",
    "from common.langchain_llm import VisionLanguageModelFactory\n",
    "from common.langchain_prompts import LangChainPromptManager\n",
    "\n",
    "# Model loaders\n",
    "from common.internvl3_model_loader import load_internvl3_model\n",
    "from common.llama_model_loader import load_llama_model\n",
    "\n",
    "# Utilities\n",
    "from common.batch_analytics import BatchAnalytics\n",
    "from common.batch_reporting import BatchReporter\n",
    "from common.batch_visualizations import BatchVisualizer\n",
    "from common.config import get_yaml_config\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "from common.extraction_parser import discover_images\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "\n",
    "console = Console()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Hybrid LangChain pipeline loaded successfully![/bold green]\")\n",
    "rprint(\"[cyan]üìã Configuration: config/models.yaml, config/prompts.yaml[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Clear GPU memory before loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">üßπ PRE-EMPTIVE GPU MEMORY CLEANUP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31müßπ PRE-EMPTIVE GPU MEMORY CLEANUP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing any existing model caches...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mClearing any existing model caches\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Running V100 emergency GPU cleanup...\n",
      "üßπ Starting V100-optimized GPU memory cleanup...\n",
      "   üìä Initial GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   ‚úÖ Final GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   üíæ Memory freed: 0.00GB\n",
      "‚úÖ V100-optimized memory cleanup complete\n",
      "‚úÖ V100 emergency cleanup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Memory cleanup complete - ready for model loading</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Memory cleanup complete - ready for model loading\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(\"[bold red]üßπ PRE-EMPTIVE GPU MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches...[/yellow]\")\n",
    "\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "**Easy model switching**: Change `MODEL_NAME` to any model from `config/models.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# MODEL SELECTION - Change this to switch models!\n# ============================================================================\n# Available models (from config/models.yaml):\n#   - \"llama-3.2-11b-vision\"         (full precision, best quality)\n#   - \"llama-3.2-11b-vision-8bit\"    (8-bit quantized, memory efficient)\n#   - \"internvl3-2b\"                 (lightweight, fast)\n#   - \"internvl3-8b\"                 (strong performance)\n#   - \"internvl3-8b-quantized\"       (V100 compatible)\n#   - \"internvl3_5-8b\"               (latest version, improved performance)\n# ============================================================================\n\nCONFIG = {\n    # Model selection (loads from config/models.yaml)\n    'MODEL_NAME': 'internvl3-8b',  # Change to switch models!\n    \n    # Prompt configuration (loads from config/prompts.yaml)\n    'SYSTEM_MODE': 'expert',  # Options: expert, structured, precise, flexible, strict\n    \n    # Data paths\n    'DATA_DIR': '/home/jovyan/nfs_share/tod/information_extractor_standalone/evaluation_data/images',  # UPDATE THIS PATH\n    'GROUND_TRUTH': '/home/jovyan/nfs_share/tod/information_extractor_standalone/evaluation_data/ground_truth.csv',  # UPDATE THIS PATH\n    'OUTPUT_BASE': './output',  # Output directory (relative to notebook location)\n    \n    # Batch settings\n    'MAX_IMAGES': None,  # None for all images\n    'DOCUMENT_TYPES': None,  # None for all types, or ['invoice', 'receipt']\n    \n    # Processing options\n    'ENABLE_PREPROCESSING': True,  # Image preprocessing\n    'PREPROCESSING_MODE': 'adaptive',  # light, moderate, aggressive, adaptive\n    'ENABLE_FIXING': False,  # Self-healing parser (experimental)\n    'ENABLE_MATH_ENHANCEMENT': False,  # Mathematical correction\n    \n    # Mode\n    'INFERENCE_ONLY': False,  # True = no ground truth needed\n    \n    # Verbosity\n    'VERBOSE': True,\n}\n\n# Adjust ground truth based on mode\nif CONFIG['INFERENCE_ONLY']:\n    CONFIG['GROUND_TRUTH'] = None\n\nrprint(\"[bold green]‚úÖ Configuration loaded[/bold green]\")\nrprint(f\"[cyan]ü§ñ Model: {CONFIG['MODEL_NAME']}[/cyan]\")\nrprint(f\"[cyan]üí¨ System Mode: {CONFIG['SYSTEM_MODE']}[/cyan]\")\nrprint(f\"[cyan]üìÇ Data: {CONFIG['DATA_DIR']}[/cyan]\")\nmode_text = 'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation'\nrprint(f\"[cyan]üéØ Mode: {mode_text}[/cyan]\")\npreprocessing_text = f\"Enabled ({CONFIG['PREPROCESSING_MODE']})\" if CONFIG['ENABLE_PREPROCESSING'] else 'Disabled'\nrprint(f\"[cyan]üîß Preprocessing: {preprocessing_text}[/cyan]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Output directories created: /home/jovyan/nfs_share/tod/information_extractor_standalone/output</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Output directories created: \u001b[0m\u001b[32m/home/jovyan/nfs_share/tod/information_extractor_standalone/\u001b[0m\u001b[32moutput\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rprint(f\"[green]‚úÖ Output directories created: {OUTPUT_BASE}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model with YAML Configuration\n",
    "\n",
    "**NEW**: Uses `VisionLanguageModelFactory` to load model from YAML config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration reloaded from /home/jovyan/nfs_share/tod/information_extractor_standalone/config\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Loading /home/jovyan/nfs_share/models/InternVL3_5-8B...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mLoading \u001b[0m\u001b[1;36m/home/jovyan/nfs_share/models/\u001b[0m\u001b[1;36mInternVL3_5-8B...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Max tokens: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Max tokens: \u001b[0m\u001b[1;36m2048\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Temperature: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Temperature: \u001b[0m\u001b[1;36m0.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Quantization: </span><span style=\"color: #008080; text-decoration-color: #008080; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Quantization: \u001b[0m\u001b[3;36mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">üöÄ Loading InternVL3 model with official optimizations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34müöÄ Loading InternVL3 model with official optimizations\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîß Configuring CUDA memory for InternVL3...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîß Configuring CUDA memory for InternVL3\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üìä Initial CUDA state <span style=\"font-weight: bold\">(</span>Multi-GPU Total<span style=\"font-weight: bold\">)</span>: <span style=\"color: #808000; text-decoration-color: #808000\">Allocated</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>00GB, <span style=\"color: #808000; text-decoration-color: #808000\">Reserved</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>00GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üìä Initial CUDA state \u001b[1m(\u001b[0mMulti-GPU Total\u001b[1m)\u001b[0m: \u001b[33mAllocated\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.\u001b[0m00GB, \u001b[33mReserved\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.\u001b[0m00GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîç Performing robust GPU memory detection...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîç Performing robust GPU memory detection\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting robust GPU memory detection...\n",
      "üìä Detected 2 GPU(s), analyzing each device...\n",
      "   GPU 0 (NVIDIA H200): 139.7GB total, 139.7GB available\n",
      "   GPU 1 (NVIDIA H200): 139.7GB total, 139.7GB available\n",
      "\n",
      "======================================================================\n",
      "üîç ROBUST GPU MEMORY DETECTION REPORT\n",
      "======================================================================\n",
      "‚úÖ Success: 2/2 GPUs detected\n",
      "üìä Total Memory: 279.44GB\n",
      "üíæ Available Memory: 279.44GB\n",
      "‚ö° Allocated Memory: 0.00GB\n",
      "üîÑ Reserved Memory: 0.00GB\n",
      "üì¶ Fragmentation: 0.00GB\n",
      "üñ•Ô∏è  Multi-GPU: Yes\n",
      "‚öñÔ∏è  Balanced Distribution: Yes\n",
      "\n",
      "üìã Per-GPU Breakdown:\n",
      "   GPU 0 (NVIDIA H200): 139.7GB total, 139.7GB available (0.0% used)\n",
      "   GPU 1 (NVIDIA H200): 139.7GB total, 139.7GB available (0.0% used)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üìä GPU Hardware: NVIDIA H200 </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">2x 140GB = 279GB total</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müìä GPU Hardware: NVIDIA H200 \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34m2x 140GB = 279GB total\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üèóÔ∏è Architecture: datacenter_high_memory </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">dynamic detection</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müèóÔ∏è Architecture: datacenter_high_memory \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mdynamic detection\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üéØ Model variant: InternVL3-8B </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">estimated need: 16GB + </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">20.</span><span style=\"color: #000080; text-decoration-color: #000080\">0GB buffer</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müéØ Model variant: InternVL3-8B \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mestimated need: 16GB + \u001b[0m\u001b[1;34m20.\u001b[0m\u001b[34m0GB buffer\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üíæ Available Memory: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">279.</span><span style=\"color: #000080; text-decoration-color: #000080\">4GB across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">GPU(</span><span style=\"color: #000080; text-decoration-color: #000080\">s</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müíæ Available Memory: \u001b[0m\u001b[1;34m279.\u001b[0m\u001b[34m4GB across \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mGPU\u001b[0m\u001b[1;34m(\u001b[0m\u001b[34ms\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üí° Memory sufficient: ‚úÖ Yes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müí° Memory sufficient: ‚úÖ Yes\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ datacenter_high_memory with 279GB - using </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">8</span><span style=\"color: #008000; text-decoration-color: #008000\">-bit quantization as configured</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ datacenter_high_memory with 279GB - using \u001b[0m\u001b[1;32m8\u001b[0m\u001b[32m-bit quantization as configured\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">üìä FINAL QUANTIZATION DECISION: ENABLED (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-bit)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36müìä FINAL QUANTIZATION DECISION: ENABLED \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;36m-bit\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Total GPU Memory: 279GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Total GPU Memory: 279GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Available Memory: 279GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Available Memory: 279GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Model needs: ~16GB + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.</span>0GB buffer for InternVL3-8B\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Model needs: ~16GB + \u001b[1;36m20.\u001b[0m0GB buffer for InternVL3-8B\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Working GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Working GPUs: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">üîß Using </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">8</span><span style=\"color: #808000; text-decoration-color: #808000\">-bit quantization </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">BitsAndBytesConfig</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\"> as per official docs</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33müîß Using \u001b[0m\u001b[1;33m8\u001b[0m\u001b[33m-bit quantization \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mBitsAndBytesConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m as per official docs\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading InternVL3 model...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mLoading InternVL3 model\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîÑ Auto-distributing model across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> GPUs...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîÑ Auto-distributing model across \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m GPUs\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">‚ö†Ô∏è Flash Attention disabled </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">V100 compatible</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m‚ö†Ô∏è Flash Attention disabled \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mV100 compatible\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ùå Failed to load model: cannot import name </span><span style=\"color: #800000; text-decoration-color: #800000\">'Qwen3Config'</span><span style=\"color: #800000; text-decoration-color: #800000\"> from </span><span style=\"color: #800000; text-decoration-color: #800000\">'transformers'</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(</span><span style=\"color: #800000; text-decoration-color: #800000\">/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/__init__.py</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m‚ùå Failed to load model: cannot import name \u001b[0m\u001b[31m'Qwen3Config'\u001b[0m\u001b[31m from \u001b[0m\u001b[31m'transformers'\u001b[0m\u001b[31m \u001b[0m\n",
       "\u001b[1;31m(\u001b[0m\u001b[31m/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/\u001b[0m\u001b[31m__init__.py\u001b[0m\u001b[1;31m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Qwen3Config' from 'transformers' (/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mMODEL_NAME\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Create LangChain wrapper with YAML config\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m llm = \u001b[43mVisionLanguageModelFactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_yaml_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMODEL_NAME\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_loader_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVERBOSE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m rprint(\u001b[33m\"\u001b[39m\u001b[33m[bold green]‚úÖ Vision-language model loaded with LangChain interface[/bold green]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m rprint(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[cyan]Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm._llm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m[/cyan]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nfs_share/tod/information_extractor_standalone/common/langchain_llm.py:490\u001b[39m, in \u001b[36mVisionLanguageModelFactory.from_yaml_config\u001b[39m\u001b[34m(model_name, model_loader_func, **override_kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    485\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel_loader_func is required. Provide a function like \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mload_llama_model or load_internvl3_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m     )\n\u001b[32m    489\u001b[39m \u001b[38;5;66;03m# Load model and processor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m model, processor = \u001b[43mmodel_loader_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# Merge YAML config with overrides\u001b[39;00m\n\u001b[32m    493\u001b[39m config_dict = model_config.to_dict()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nfs_share/tod/information_extractor_standalone/common/internvl3_model_loader.py:391\u001b[39m, in \u001b[36mload_internvl3_model\u001b[39m\u001b[34m(model_path, use_quantization, device_map, max_new_tokens, torch_dtype, low_cpu_mem_usage, use_flash_attn, verbose)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[32m    389\u001b[39m     rprint(\u001b[33m\"\u001b[39m\u001b[33m[yellow]‚ö†Ô∏è Flash Attention disabled (V100 compatible)[/yellow]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:526\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    524\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1020\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(pretrained_model_name_or_path):\n\u001b[32m   1019\u001b[39m         config_class.register_for_auto_class()\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/configuration_utils.py:548\u001b[39m, in \u001b[36mPretrainedConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[38;5;28mcls\u001b[39m.model_type:\n\u001b[32m    543\u001b[39m     logger.warning(\n\u001b[32m    544\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to instantiate a model of type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.model_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    546\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/configuration_utils.py:714\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    712\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    717\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/InternVL3_5-8B/configuration_internvl_chat.py:67\u001b[39m, in \u001b[36mInternVLChatConfig.__init__\u001b[39m\u001b[34m(self, vision_config, llm_config, use_backbone_lora, use_llm_lora, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm_config = Qwen3MoeConfig(**llm_config)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m architecture == \u001b[33m'\u001b[39m\u001b[33mQwen3ForCausalLM\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen3Config\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mself\u001b[39m.llm_config = Qwen3Config(**llm_config)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Qwen3Config' from 'transformers' (/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Load YAML configuration\n",
    "yaml_config = get_yaml_config()\n",
    "model_config = yaml_config.get_model_config(CONFIG['MODEL_NAME'])\n",
    "\n",
    "rprint(f\"[bold cyan]Loading {model_config.model_id}...[/bold cyan]\")\n",
    "rprint(f\"[cyan]  Max tokens: {model_config.max_new_tokens}[/cyan]\")\n",
    "rprint(f\"[cyan]  Temperature: {model_config.temperature}[/cyan]\")\n",
    "rprint(f\"[cyan]  Quantization: {model_config.use_quantization}[/cyan]\")\n",
    "\n",
    "# Select appropriate model loader based on model name\n",
    "if 'llama' in CONFIG['MODEL_NAME'].lower():\n",
    "    model_loader = load_llama_model\n",
    "elif 'internvl' in CONFIG['MODEL_NAME'].lower():\n",
    "    model_loader = load_internvl3_model\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {CONFIG['MODEL_NAME']}\")\n",
    "\n",
    "# Create LangChain wrapper with YAML config\n",
    "llm = VisionLanguageModelFactory.from_yaml_config(\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    model_loader_func=model_loader,\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Vision-language model loaded with LangChain interface[/bold green]\")\n",
    "rprint(f\"[cyan]Type: {llm._llm_type}[/cyan]\")\n",
    "rprint(f\"[cyan]Model: {llm.model_id}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Processing Pipeline\n",
    "\n",
    "**NEW**: Uses LangChain `DocumentProcessingPipeline` with YAML prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processing pipeline\n",
    "pipeline = create_pipeline(\n",
    "    llm=llm,\n",
    "    enable_fixing=CONFIG['ENABLE_FIXING'],\n",
    "    enable_cleaning=False,  # Disable automatic cleaning - we'll do it explicitly in a separate cell\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Create prompt manager for hot-reload capability\n",
    "prompt_manager = LangChainPromptManager(\n",
    "    use_yaml_config=True,\n",
    "    system_mode=CONFIG['SYSTEM_MODE']\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Processing pipeline created[/bold green]\")\n",
    "rprint(\"[cyan]Features:[/cyan]\")\n",
    "rprint(\"  ‚úì Document type detection\")\n",
    "rprint(\"  ‚úì Field extraction with Pydantic validation\")\n",
    "rprint(\"  ‚úì Cleaning disabled - will be done explicitly in separate cell\")\n",
    "rprint(\"  ‚úì Hot-reload prompts: prompt_manager.reload_config()\")\n",
    "\n",
    "if CONFIG['ENABLE_FIXING']:\n",
    "    rprint(\"  ‚úì Self-healing parser enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image Discovery and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover images\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Image preprocessing (optional)\n",
    "if CONFIG['ENABLE_PREPROCESSING']:\n",
    "    import tempfile\n",
    "    from common.image_preprocessing import (\n",
    "        enhance_statement_quality,\n",
    "        enhance_for_llama,\n",
    "        preprocess_statement_for_llama,\n",
    "        adaptive_enhance,\n",
    "        preprocess_recommended\n",
    "    )\n",
    "    \n",
    "    preprocess_functions = {\n",
    "        'light': enhance_statement_quality,\n",
    "        'moderate': enhance_for_llama,\n",
    "        'aggressive': preprocess_statement_for_llama,\n",
    "        'adaptive': adaptive_enhance,\n",
    "        'recommended': preprocess_recommended\n",
    "    }\n",
    "    \n",
    "    preprocess_fn = preprocess_functions[CONFIG['PREPROCESSING_MODE']]\n",
    "    preprocessed_images = []\n",
    "    preprocessed_dir = Path(tempfile.mkdtemp(prefix='preprocessed_'))\n",
    "    \n",
    "    rprint(f\"[cyan]üîß Preprocessing {len(all_images)} images ({CONFIG['PREPROCESSING_MODE']})[/cyan]\")\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        try:\n",
    "            preprocessed_img = preprocess_fn(img_path)\n",
    "            preprocessed_path = preprocessed_dir / Path(img_path).name\n",
    "            preprocessed_img.save(preprocessed_path)\n",
    "            preprocessed_images.append(str(preprocessed_path))\n",
    "        except Exception as e:\n",
    "            rprint(f\"[yellow]‚ö†Ô∏è Preprocessing failed for {Path(img_path).name}: {e}[/yellow]\")\n",
    "            preprocessed_images.append(img_path)\n",
    "    \n",
    "    all_images = preprocessed_images\n",
    "    rprint(f\"[green]‚úÖ Preprocessing complete[/green]\")\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "\n",
    "# Apply filters\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing with LangChain Pipeline\n",
    "\n",
    "**NEW**: Uses `pipeline.process_batch()` with LangChain interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batch using LangChain pipeline\n",
    "rprint(\"[bold cyan]üöÄ Starting batch processing...[/bold cyan]\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "batch_results = pipeline.process_batch(all_images)\n",
    "end_time = datetime.now()\n",
    "\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "processing_times = [total_time / len(batch_results)] * len(batch_results)  # Estimate individual times\n",
    "\n",
    "# Extract document types\n",
    "document_types_found = {}\n",
    "for result in batch_results:\n",
    "    doc_type = result.get('document_type', 'unknown')\n",
    "    document_types_found[doc_type] = document_types_found.get(doc_type, 0) + 1\n",
    "\n",
    "# Summary\n",
    "rprint(f\"[bold green]‚úÖ Processed {len(batch_results)} images in {total_time:.1f}s[/bold green]\")\n",
    "rprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s per image[/cyan]\")\n",
    "rprint(f\"[cyan]Document types found: {document_types_found}[/cyan]\")\n",
    "\n",
    "# Count successful vs failed\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "failed = len(batch_results) - successful\n",
    "rprint(f\"[cyan]Success rate: {successful}/{len(batch_results)} ({successful/len(batch_results)*100:.1f}%)[/cyan]\")\n",
    "if failed > 0:\n",
    "    rprint(f\"[yellow]‚ö†Ô∏è  {failed} documents had processing errors[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Clean and Normalize Extracted Data\n",
    "\n",
    "**CRITICAL**: Apply ExtractionCleaner to normalize field values for accuracy.\n",
    "\n",
    "This step transforms raw model output into clean, ground-truth-compatible format:\n",
    "- **ABN**: `06082698025` ‚Üí `06 082 698 025`\n",
    "- **Monetary**: `$4,834.03` ‚Üí `$4834.03` (remove commas)\n",
    "- **Addresses**: Remove phone/email, normalize spacing\n",
    "- **Dates**: `Thu 04 Sep 2025` ‚Üí `04/09/2025`\n",
    "- **Document types**: `STATEMENT` ‚Üí `BANK_STATEMENT`\n",
    "- **Lists**: Convert to pipe-separated format\n",
    "\n",
    "**Expected impact**: Restores accuracy from ~49% ‚Üí ~82% (81.8% baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "from common.extraction_cleaner import ExtractionCleaner\n",
    "\n",
    "\n",
    "def serialize_pydantic_to_llm_format(field_name: str, value) -> str:\n",
    "    \"\"\"\n",
    "    Convert Pydantic model types to LLM output format strings.\n",
    "    \n",
    "    This prepares types for ExtractionCleaner which expects LLM string output.\n",
    "    \"\"\"\n",
    "    # Handle NOT_FOUND\n",
    "    if value == \"NOT_FOUND\" or value is None:\n",
    "        return \"NOT_FOUND\"\n",
    "    \n",
    "    # Handle lists - convert to comma-separated (cleaner will convert to pipes)\n",
    "    if isinstance(value, list):\n",
    "        if not value:\n",
    "            return \"NOT_FOUND\"\n",
    "        # Convert each item to string\n",
    "        str_items = []\n",
    "        for item in value:\n",
    "            if isinstance(item, Decimal):\n",
    "                str_items.append(f\"${str(item)}\")\n",
    "            else:\n",
    "                str_items.append(str(item))\n",
    "        return \", \".join(str_items)\n",
    "    \n",
    "    # Handle Decimal - convert to string with $ for monetary fields\n",
    "    if isinstance(value, Decimal):\n",
    "        return f\"${str(value)}\"\n",
    "    \n",
    "    # Handle boolean - convert to lowercase string\n",
    "    if isinstance(value, bool):\n",
    "        return str(value).lower()\n",
    "    \n",
    "    # Everything else - just convert to string\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "# Create cleaner instance\n",
    "cleaner = ExtractionCleaner(debug=False)  # Set to True to see each field transformation\n",
    "\n",
    "rprint(\"[bold cyan]üßπ Cleaning and normalizing extracted data...[/bold cyan]\")\n",
    "\n",
    "# Track cleaning statistics\n",
    "cleaning_stats = {\n",
    "    'documents_cleaned': 0,\n",
    "    'total_fields_cleaned': 0,\n",
    "    'cleaning_examples': []\n",
    "}\n",
    "\n",
    "# Clean each document's extracted data\n",
    "for i, result in enumerate(batch_results):\n",
    "    if 'extracted_data' not in result:\n",
    "        continue\n",
    "    \n",
    "    extracted_data = result['extracted_data']\n",
    "    \n",
    "    # Convert Pydantic model to dict\n",
    "    if hasattr(extracted_data, 'model_dump'):\n",
    "        raw_dict = extracted_data.model_dump()\n",
    "    elif hasattr(extracted_data, 'dict'):\n",
    "        raw_dict = extracted_data.dict()\n",
    "    else:\n",
    "        continue  # Skip if not a Pydantic model\n",
    "    \n",
    "    # CRITICAL: Serialize Python types to LLM format strings\n",
    "    # This converts Lists‚Üístrings, Decimals‚Üí\"$123.45\", bools‚Üí\"true\"/\"false\"\n",
    "    serialized_dict = {\n",
    "        field: serialize_pydantic_to_llm_format(field, value)\n",
    "        for field, value in raw_dict.items()\n",
    "    }\n",
    "    \n",
    "    # Clean all fields (now expects LLM string output)\n",
    "    cleaned_dict = cleaner.clean_extraction_dict(serialized_dict)\n",
    "    \n",
    "    # Count changes\n",
    "    changes_made = sum(1 for field in serialized_dict if str(serialized_dict[field]) != str(cleaned_dict[field]))\n",
    "    \n",
    "    # Store first few examples for display\n",
    "    if changes_made > 0 and len(cleaning_stats['cleaning_examples']) < 3:\n",
    "        doc_example = {\n",
    "            'image_name': result.get('image_name', 'unknown'),\n",
    "            'document_type': result.get('document_type', 'unknown'),\n",
    "            'fields_changed': changes_made,\n",
    "            'examples': []\n",
    "        }\n",
    "        \n",
    "        for field in serialized_dict:\n",
    "            raw_value = str(serialized_dict[field])\n",
    "            cleaned_value = str(cleaned_dict[field])\n",
    "            if raw_value != cleaned_value and len(doc_example['examples']) < 3:\n",
    "                doc_example['examples'].append({\n",
    "                    'field': field,\n",
    "                    'raw': raw_value[:80] + '...' if len(raw_value) > 80 else raw_value,\n",
    "                    'cleaned': cleaned_value[:80] + '...' if len(cleaned_value) > 80 else cleaned_value\n",
    "                })\n",
    "        \n",
    "        if doc_example['examples']:\n",
    "            cleaning_stats['cleaning_examples'].append(doc_example)\n",
    "    \n",
    "    cleaning_stats['total_fields_cleaned'] += changes_made\n",
    "    cleaning_stats['documents_cleaned'] += 1\n",
    "    \n",
    "    # Store cleaned dict directly (preserves pipe-separated strings)\n",
    "    result['extracted_data'] = cleaned_dict\n",
    "    result['cleaning_applied'] = True\n",
    "\n",
    "# Display cleaning summary\n",
    "rprint(f\"[green]‚úÖ Cleaned {cleaning_stats['documents_cleaned']} documents[/green]\")\n",
    "rprint(f\"[cyan]Total field transformations: {cleaning_stats['total_fields_cleaned']}[/cyan]\")\n",
    "rprint(f\"[cyan]Average per document: {cleaning_stats['total_fields_cleaned']/cleaning_stats['documents_cleaned']:.1f} fields[/cyan]\")\n",
    "\n",
    "# Show examples\n",
    "if cleaning_stats['cleaning_examples']:\n",
    "    rprint(\"\\n[bold yellow]üìã Cleaning Examples:[/bold yellow]\")\n",
    "    for example in cleaning_stats['cleaning_examples']:\n",
    "        rprint(f\"\\n[yellow]{example['image_name']} ({example['document_type']}) - {example['fields_changed']} fields changed:[/yellow]\")\n",
    "        for ex in example['examples']:\n",
    "            rprint(f\"  [cyan]{ex['field']}:[/cyan]\")\n",
    "            rprint(f\"    Raw:     {ex['raw']}\")\n",
    "            rprint(f\"    Cleaned: {ex['cleaned']}\")\n",
    "else:\n",
    "    rprint(\"\\n[dim]No field transformations needed - all values already clean[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Evaluation Against Ground Truth\n",
    "\n",
    "**NEW**: Calculate accuracy metrics by comparing extracted data with ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for each result (if in evaluation mode)\n",
    "if not CONFIG['INFERENCE_ONLY'] and ground_truth:\n",
    "    from common.config import get_document_type_fields\n",
    "    from common.evaluation_metrics import calculate_field_accuracy_with_method\n",
    "    \n",
    "    rprint(\"[cyan]üîç Calculating evaluation metrics...[/cyan]\")\n",
    "    \n",
    "    # DEBUG: Track what fields are being used per document type\n",
    "    fields_per_doc_type = {}\n",
    "    \n",
    "    # DEBUG: Show first document comparison\n",
    "    debug_shown = False\n",
    "    \n",
    "    for result in batch_results:\n",
    "        img_name = result['image_name']\n",
    "        \n",
    "        if img_name not in ground_truth:\n",
    "            result['evaluation'] = {'inference_only': True}\n",
    "            continue\n",
    "        \n",
    "        # Get extracted data as dict\n",
    "        extracted_data = result.get('extracted_data', {})\n",
    "        if hasattr(extracted_data, 'model_dump'):\n",
    "            extracted_dict = extracted_data.model_dump()\n",
    "        elif hasattr(extracted_data, 'dict'):\n",
    "            extracted_dict = extracted_data.dict()\n",
    "        else:\n",
    "            extracted_dict = extracted_data if isinstance(extracted_data, dict) else {}\n",
    "        \n",
    "        # Get ground truth for this image\n",
    "        truth_data = ground_truth[img_name]\n",
    "        doc_type = result.get('document_type', '')\n",
    "        \n",
    "        # Get fields for this document type\n",
    "        doc_fields = get_document_type_fields(doc_type)\n",
    "        \n",
    "        # DEBUG: Show detailed comparison for first document\n",
    "        if not debug_shown and doc_type == 'receipt':\n",
    "            rprint(f\"\\n[bold yellow]üîç DEBUG: Comparing {img_name} ({doc_type}):[/bold yellow]\")\n",
    "            rprint(f\"[yellow]Fields to evaluate: {doc_fields}[/yellow]\")\n",
    "            for field in doc_fields[:3]:  # Show first 3 fields\n",
    "                ext_val = extracted_dict.get(field, 'NOT_FOUND')\n",
    "                truth_val = truth_data.get(field, 'NOT_FOUND')\n",
    "                rprint(f\"\\n[cyan]{field}:[/cyan]\")\n",
    "                rprint(f\"  Extracted: {ext_val!r} (type: {type(ext_val).__name__})\")\n",
    "                rprint(f\"  Truth:     {truth_val!r} (type: {type(truth_val).__name__})\")\n",
    "                rprint(f\"  Match: {ext_val == truth_val}\")\n",
    "            debug_shown = True\n",
    "        \n",
    "        # DEBUG: Track fields used per document type\n",
    "        if doc_type not in fields_per_doc_type:\n",
    "            fields_per_doc_type[doc_type] = {\n",
    "                'field_count': len(doc_fields),\n",
    "                'fields': doc_fields,\n",
    "                'sample_count': 0\n",
    "            }\n",
    "        fields_per_doc_type[doc_type]['sample_count'] += 1\n",
    "        \n",
    "        # Calculate field-level accuracies\n",
    "        field_accuracies = {}\n",
    "        total_fields = 0\n",
    "        fields_matched = 0\n",
    "        fields_extracted = 0\n",
    "        \n",
    "        for field in doc_fields:\n",
    "            total_fields += 1\n",
    "            extracted_value = extracted_dict.get(field, 'NOT_FOUND')\n",
    "            truth_value = truth_data.get(field, 'NOT_FOUND')\n",
    "            \n",
    "            # Count extracted fields\n",
    "            if extracted_value != 'NOT_FOUND':\n",
    "                fields_extracted += 1\n",
    "            \n",
    "            # Calculate accuracy for this field (returns dict with f1_score, precision, recall, etc.)\n",
    "            accuracy_result = calculate_field_accuracy_with_method(\n",
    "                extracted_value,\n",
    "                truth_value,\n",
    "                field,\n",
    "                method='order_aware_f1',  # Use configured method\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            # Extract f1_score from the result dict\n",
    "            accuracy_score = accuracy_result.get('f1_score', 0.0) if isinstance(accuracy_result, dict) else float(accuracy_result)\n",
    "            \n",
    "            field_accuracies[field] = {\n",
    "                'accuracy': accuracy_score,\n",
    "                'extracted': str(extracted_value),\n",
    "                'ground_truth': str(truth_value)\n",
    "            }\n",
    "            \n",
    "            # Count matched fields (accuracy > 0)\n",
    "            if accuracy_score > 0:\n",
    "                fields_matched += 1\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        overall_accuracy = sum(fa['accuracy'] for fa in field_accuracies.values()) / total_fields if total_fields > 0 else 0\n",
    "        \n",
    "        # Store evaluation results\n",
    "        result['evaluation'] = {\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'fields_extracted': fields_extracted,\n",
    "            'fields_matched': fields_matched,\n",
    "            'total_fields': total_fields,\n",
    "            'field_accuracies': field_accuracies,\n",
    "            'inference_only': False\n",
    "        }\n",
    "    \n",
    "    rprint(\"[green]‚úÖ Evaluation metrics calculated[/green]\")\n",
    "    \n",
    "    # DEBUG: Show what fields were used per document type\n",
    "    rprint(\"\\n[bold yellow]üîç Debug: Fields used per document type:[/bold yellow]\")\n",
    "    for doc_type, info in fields_per_doc_type.items():\n",
    "        rprint(f\"[yellow]  {doc_type}: {info['field_count']} fields ({info['sample_count']} documents)[/yellow]\")\n",
    "        rprint(f\"[dim]    Fields: {', '.join(info['fields'][:5])}{'...' if len(info['fields']) > 5 else ''}[/dim]\")\n",
    "    \n",
    "    # Show average accuracy\n",
    "    evaluated = [r for r in batch_results if not r.get('evaluation', {}).get('inference_only', True)]\n",
    "    if evaluated:\n",
    "        avg_accuracy = np.mean([r['evaluation']['overall_accuracy'] * 100 for r in evaluated])\n",
    "        rprint(f\"\\n[bold cyan]üìä Average Accuracy: {avg_accuracy:.1f}%[/bold cyan]\")\n",
    "else:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Skipping evaluation - inference-only mode or no ground truth[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Analytics and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field columns dynamically from config (excluding validation-only fields)\n",
    "from common.config import get_extraction_fields, get_validation_only_fields\n",
    "\n",
    "# Get all extraction fields\n",
    "all_fields = get_extraction_fields()\n",
    "\n",
    "# Get validation-only fields to exclude\n",
    "validation_only = get_validation_only_fields()\n",
    "\n",
    "# Filter out validation-only fields\n",
    "FIELD_COLUMNS = [f for f in all_fields if f not in validation_only]\n",
    "\n",
    "rprint(f\"[cyan]üìã Using {len(FIELD_COLUMNS)} evaluation fields (excluded {len(validation_only)} validation-only)[/cyan]\")\n",
    "if validation_only:\n",
    "    rprint(f\"[dim]Excluded: {', '.join(validation_only)}[/dim]\")\n",
    "\n",
    "# Add processing_time and prompt_used to batch_results for BatchAnalytics\n",
    "for i, result in enumerate(batch_results):\n",
    "    result['processing_time'] = processing_times[i]\n",
    "    result['prompt_used'] = f\"{CONFIG['SYSTEM_MODE']}_mode\"\n",
    "    # Add evaluation stub if not present (for inference-only mode)\n",
    "    if 'evaluation' not in result:\n",
    "        result['evaluation'] = {'inference_only': True}\n",
    "\n",
    "# Create CSV data\n",
    "csv_data = []\n",
    "for i, result in enumerate(batch_results):\n",
    "    image_name = Path(result['image_path']).name\n",
    "    doc_type = result.get('document_type', '').lower()\n",
    "    extracted_data = result.get('extracted_data', {})\n",
    "    \n",
    "    # Convert Pydantic model to dict if needed\n",
    "    if hasattr(extracted_data, 'model_dump'):\n",
    "        extracted_fields = extracted_data.model_dump()\n",
    "    elif hasattr(extracted_data, 'dict'):\n",
    "        extracted_fields = extracted_data.dict()\n",
    "    else:\n",
    "        extracted_fields = extracted_data if isinstance(extracted_data, dict) else {}\n",
    "    \n",
    "    # Count fields\n",
    "    found_fields = sum(1 for field in FIELD_COLUMNS \n",
    "                      if extracted_fields.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n",
    "    field_coverage = (found_fields / len(FIELD_COLUMNS) * 100)\n",
    "    \n",
    "    # Evaluation data\n",
    "    evaluation = result.get('evaluation', {})\n",
    "    if CONFIG['INFERENCE_ONLY'] or not evaluation or evaluation.get('inference_only'):\n",
    "        overall_accuracy = None\n",
    "    else:\n",
    "        overall_accuracy = evaluation.get('overall_accuracy', 0) * 100\n",
    "    \n",
    "    row_data = {\n",
    "        'image_file': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_times[i],\n",
    "        'found_fields': found_fields,\n",
    "        'field_coverage': field_coverage,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'model_used': CONFIG['MODEL_NAME'],\n",
    "        'system_mode': CONFIG['SYSTEM_MODE'],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    # Add field values\n",
    "    for field in FIELD_COLUMNS:\n",
    "        row_data[field] = extracted_fields.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    csv_data.append(row_data)\n",
    "\n",
    "# Create and save DataFrame\n",
    "model_short_name = CONFIG['MODEL_NAME'].split('-')[0]  # llama or internvl3\n",
    "results_df = pd.DataFrame(csv_data)\n",
    "csv_path = OUTPUT_DIRS['csv'] / f\"{model_short_name}_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Results exported to CSV[/bold green]\")\n",
    "rprint(f\"[cyan]üìÑ File: {csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]üìä Structure: {len(results_df)} rows √ó {len(results_df.columns)} columns[/cyan]\")\n",
    "\n",
    "# Display sample\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', 'field_coverage']\n",
    "else:\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', 'found_fields']\n",
    "\n",
    "rprint(\"\\n[bold blue]üìã Sample Results:[/bold blue]\")\n",
    "display(results_df[sample_cols].head(3))\n",
    "\n",
    "# Create analytics\n",
    "analytics = BatchAnalytics(batch_results, processing_times)\n",
    "saved_files, df_results, df_summary, df_doctype_stats, df_field_stats = analytics.save_all_dataframes(\n",
    "    OUTPUT_DIRS['csv'], BATCH_TIMESTAMP, verbose=False\n",
    ")\n",
    "\n",
    "rprint(\"\\n[bold blue]üìä Results Summary[/bold blue]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(f\"[cyan]‚úÖ Total images: {len(batch_results)}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Avg fields found: {results_df['found_fields'].mean():.1f}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Avg coverage: {results_df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "else:\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = BatchVisualizer()\n",
    "\n",
    "viz_files = visualizer.create_all_visualizations(\n",
    "    df_results,\n",
    "    df_doctype_stats,\n",
    "    OUTPUT_DIRS['visualizations'],\n",
    "    BATCH_TIMESTAMP,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "rprint(f\"[green]‚úÖ Visualizations saved to {OUTPUT_DIRS['visualizations']}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = BatchReporter(\n",
    "    batch_results,\n",
    "    processing_times,\n",
    "    document_types_found,\n",
    "    BATCH_TIMESTAMP\n",
    ")\n",
    "\n",
    "report_files = reporter.save_all_reports(\n",
    "    OUTPUT_DIRS,\n",
    "    df_results,\n",
    "    df_summary,\n",
    "    df_doctype_stats,\n",
    "    CONFIG['MODEL_NAME'],\n",
    "    {\n",
    "        'data_dir': CONFIG['DATA_DIR'],\n",
    "        'ground_truth': CONFIG['GROUND_TRUTH'],\n",
    "        'max_images': CONFIG['MAX_IMAGES'],\n",
    "    },\n",
    "    {\n",
    "        'model_name': CONFIG['MODEL_NAME'],\n",
    "        'system_mode': CONFIG['SYSTEM_MODE'],\n",
    "        'enable_fixing': CONFIG['ENABLE_FIXING'],\n",
    "        'preprocessing': CONFIG['PREPROCESSING_MODE'] if CONFIG['ENABLE_PREPROCESSING'] else 'disabled'\n",
    "    },\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "rprint(f\"[green]‚úÖ Reports saved to {OUTPUT_DIRS['reports']}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Final Summary and Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.rule(\"[bold green]Processing Complete[/bold green]\")\n",
    "\n",
    "total_images = len(batch_results)\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "\n",
    "rprint(f\"[bold green]‚úÖ Processed: {total_images} images[/bold green]\")\n",
    "rprint(f\"[cyan]Success Rate: {(successful/total_images*100):.1f}%[/cyan]\")\n",
    "\n",
    "if not CONFIG['INFERENCE_ONLY']:\n",
    "    avg_accuracy = results_df['overall_accuracy'].mean() if 'overall_accuracy' in results_df else 0\n",
    "    rprint(f\"[cyan]Average Accuracy: {avg_accuracy:.2f}%[/cyan]\")\n",
    "\n",
    "rprint(f\"[cyan]Model: {CONFIG['MODEL_NAME']}[/cyan]\")\n",
    "rprint(f\"[cyan]System Mode: {CONFIG['SYSTEM_MODE']}[/cyan]\")\n",
    "rprint(f\"[cyan]Output: {OUTPUT_BASE}[/cyan]\")\n",
    "\n",
    "# Display dashboard\n",
    "dashboard_files = list(OUTPUT_DIRS['visualizations'].glob(f\"dashboard_{BATCH_TIMESTAMP}.png\"))\n",
    "if dashboard_files:\n",
    "    rprint(\"\\n[bold blue]üìä Visual Dashboard:[/bold blue]\")\n",
    "    display(Image(str(dashboard_files[0])))\n",
    "\n",
    "# Hot-reload info\n",
    "rprint(\"\\n[bold cyan]üí° Hot-Reload Tips:[/bold cyan]\")\n",
    "rprint(\"  ‚Ä¢ Edit prompts: config/prompts.yaml\")\n",
    "rprint(\"  ‚Ä¢ Reload: prompt_manager.reload_config()\")\n",
    "rprint(\"  ‚Ä¢ Switch models: Change CONFIG['MODEL_NAME'] and rerun Section 5\")\n",
    "rprint(\"  ‚Ä¢ Change system mode: Edit CONFIG['SYSTEM_MODE'] (expert/precise/flexible/strict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Zero Accuracy Analysis (Evaluation Mode Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CONFIG['INFERENCE_ONLY']:\n",
    "    zero_accuracy_count = 0\n",
    "    zero_accuracy_images = []\n",
    "    total_evaluated = 0\n",
    "    \n",
    "    for result in batch_results:\n",
    "        evaluation = result.get('evaluation', {})\n",
    "        if evaluation and not evaluation.get('inference_only', False):\n",
    "            total_evaluated += 1\n",
    "            accuracy = evaluation.get('overall_accuracy', 0)\n",
    "            \n",
    "            if accuracy == 0.0:\n",
    "                zero_accuracy_count += 1\n",
    "                zero_accuracy_images.append({\n",
    "                    'image_name': result.get('image_name', 'unknown'),\n",
    "                    'document_type': result.get('document_type', 'unknown'),\n",
    "                    'fields_extracted': evaluation.get('fields_extracted', 0),\n",
    "                    'total_fields': evaluation.get('total_fields', 0),\n",
    "                })\n",
    "    \n",
    "    console.rule(\"[bold red]Zero Accuracy Analysis[/bold red]\")\n",
    "    rprint(f\"[cyan]Total documents evaluated: {total_evaluated}[/cyan]\")\n",
    "    rprint(f\"[red]Documents with 0% accuracy: {zero_accuracy_count}[/red]\")\n",
    "    \n",
    "    if zero_accuracy_count > 0:\n",
    "        percentage = (zero_accuracy_count / total_evaluated) * 100\n",
    "        rprint(f\"[red]Zero accuracy rate: {percentage:.1f}%[/red]\")\n",
    "        rprint(\"\\n[bold red]Documents with 0% Accuracy:[/bold red]\")\n",
    "        for i, img_info in enumerate(zero_accuracy_images, 1):\n",
    "            rprint(f\"  {i}. {img_info['image_name']} ({img_info['document_type']})\")\n",
    "            rprint(f\"     Fields: {img_info['fields_extracted']}/{img_info['total_fields']}\")\n",
    "    else:\n",
    "        rprint(\"[green]‚úÖ No documents with 0% accuracy - all extractions had some success![/green]\")\n",
    "else:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Zero accuracy analysis requires evaluation mode[/yellow]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}