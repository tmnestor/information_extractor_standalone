{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extractor - Hybrid LangChain Pipeline\n",
    "\n",
    "**Streamlined document extraction using YAML-configured LangChain v1.0 pipeline.**\n",
    "\n",
    "**Key Features:**\n",
    "- YAML-based configuration (no code changes needed)\n",
    "- Hot-reload capability (edit prompts live)\n",
    "- LangChain v1.0 (BaseChatModel with multi-modal messages)\n",
    "- Easy model switching (Llama â†” InternVL3)\n",
    "- Comprehensive analytics and visualizations\n",
    "- Production-ready validation and cleaning (81.8% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… Hybrid LangChain pipeline loaded successfully!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ… Hybrid LangChain pipeline loaded successfully!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ“‹ Configuration: config/models.yaml, config/prompts.yaml</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ“‹ Configuration: config/models.yaml, config/prompts.yaml\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'correlation'\n",
    "\n",
    "# Standard library\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "# LangChain Integration\n",
    "from common.langchain_chains import create_pipeline\n",
    "from common.langchain_llm import VisionLanguageModelFactory\n",
    "from common.langchain_prompts import LangChainPromptManager\n",
    "\n",
    "# Model loaders\n",
    "from common.internvl3_model_loader import load_internvl3_model\n",
    "from common.llama_model_loader import load_llama_model\n",
    "\n",
    "# Utilities\n",
    "from common.batch_analytics import BatchAnalytics\n",
    "from common.batch_reporting import BatchReporter\n",
    "from common.batch_visualizations import BatchVisualizer\n",
    "from common.config import get_yaml_config\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "from common.extraction_parser import discover_images\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "\n",
    "console = Console()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rprint(\"[bold green]âœ… Hybrid LangChain pipeline loaded successfully![/bold green]\")\n",
    "rprint(\"[cyan]ğŸ“‹ Configuration: config/models.yaml, config/prompts.yaml[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Clear GPU memory before loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ğŸ§¹ PRE-EMPTIVE GPU MEMORY CLEANUP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mğŸ§¹ PRE-EMPTIVE GPU MEMORY CLEANUP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing any existing model caches...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mClearing any existing model caches\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Running V100 emergency GPU cleanup...\n",
      "ğŸ§¹ Starting V100-optimized GPU memory cleanup...\n",
      "   ğŸ“Š Initial GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   âœ… Final GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   ğŸ’¾ Memory freed: 0.00GB\n",
      "âœ… V100-optimized memory cleanup complete\n",
      "âœ… V100 emergency cleanup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Memory cleanup complete - ready for model loading</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Memory cleanup complete - ready for model loading\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(\"[bold red]ğŸ§¹ PRE-EMPTIVE GPU MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches...[/yellow]\")\n",
    "\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]âœ… Memory cleanup complete - ready for model loading[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "**Easy model switching**: Change `MODEL_NAME` to any model from `config/models.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… Configuration loaded</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ… Configuration loaded\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ¤– Model: llama-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span><span style=\"color: #008080; text-decoration-color: #008080\">-11b-vision</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ¤– Model: llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[36m-11b-vision\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ’¬ System Mode: expert</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ’¬ System Mode: expert\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ“‚ Data: /home/jovyan/nfs_share/tod/information_extractor_standalone/evaluation_data/images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ“‚ Data: \u001b[0m\u001b[36m/home/jovyan/nfs_share/tod/information_extractor_standalone/evaluation_data/\u001b[0m\u001b[36mimages\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ¯ Mode: Evaluation</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ¯ Mode: Evaluation\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ”§ Preprocessing: Enabled </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">adaptive</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ”§ Preprocessing: Enabled \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36madaptive\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL SELECTION - Change this to switch models!\n",
    "# ============================================================================\n",
    "# Available models (from config/models.yaml):\n",
    "#   - \"llama-3.2-11b-vision\"         (full precision, best quality)\n",
    "#   - \"llama-3.2-11b-vision-8bit\"    (8-bit quantized, memory efficient)\n",
    "#   - \"internvl3-2b\"                 (lightweight, fast)\n",
    "#   - \"internvl3-8b\"                 (strong performance)\n",
    "#   - \"internvl3-8b-quantized\"       (V100 compatible)\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model selection (loads from config/models.yaml)\n",
    "    'MODEL_NAME': 'llama-3.2-11b-vision',  # Change to switch models!\n",
    "    \n",
    "    # Prompt configuration (loads from config/prompts.yaml)\n",
    "    'SYSTEM_MODE': 'expert',  # Options: expert, structured, precise, flexible, strict\n",
    "    \n",
    "    # Data paths\n",
    "    'DATA_DIR': '/home/jovyan/nfs_share/tod/information_extractor_standalone/evaluation_data/images',  # UPDATE THIS PATH\n",
    "    'GROUND_TRUTH': '/home/jovyan/nfs_share/tod/information_extractor_standalone/evaluation_data/ground_truth.csv',  # UPDATE THIS PATH\n",
    "    'OUTPUT_BASE': './output',  # Output directory (relative to notebook location)\n",
    "    \n",
    "    # Batch settings\n",
    "    'MAX_IMAGES': None,  # None for all images\n",
    "    'DOCUMENT_TYPES': None,  # None for all types, or ['invoice', 'receipt']\n",
    "    \n",
    "    # Processing options\n",
    "    'ENABLE_PREPROCESSING': True,  # Image preprocessing\n",
    "    'PREPROCESSING_MODE': 'adaptive',  # light, moderate, aggressive, adaptive\n",
    "    'ENABLE_FIXING': False,  # Self-healing parser (experimental)\n",
    "    'ENABLE_MATH_ENHANCEMENT': False,  # Mathematical correction\n",
    "    \n",
    "    # Mode\n",
    "    'INFERENCE_ONLY': False,  # True = no ground truth needed\n",
    "    \n",
    "    # Verbosity\n",
    "    'VERBOSE': True,\n",
    "}\n",
    "\n",
    "# Adjust ground truth based on mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "rprint(\"[bold green]âœ… Configuration loaded[/bold green]\")\n",
    "rprint(f\"[cyan]ğŸ¤– Model: {CONFIG['MODEL_NAME']}[/cyan]\")\n",
    "rprint(f\"[cyan]ğŸ’¬ System Mode: {CONFIG['SYSTEM_MODE']}[/cyan]\")\n",
    "rprint(f\"[cyan]ğŸ“‚ Data: {CONFIG['DATA_DIR']}[/cyan]\")\n",
    "mode_text = 'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation'\n",
    "rprint(f\"[cyan]ğŸ¯ Mode: {mode_text}[/cyan]\")\n",
    "preprocessing_text = f\"Enabled ({CONFIG['PREPROCESSING_MODE']})\" if CONFIG['ENABLE_PREPROCESSING'] else 'Disabled'\n",
    "rprint(f\"[cyan]ğŸ”§ Preprocessing: {preprocessing_text}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Output directories created: /home/jovyan/nfs_share/tod/information_extractor_standalone/output</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Output directories created: \u001b[0m\u001b[32m/home/jovyan/nfs_share/tod/information_extractor_standalone/\u001b[0m\u001b[32moutput\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rprint(f\"[green]âœ… Output directories created: {OUTPUT_BASE}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model with YAML Configuration\n",
    "\n",
    "**NEW**: Uses `VisionLanguageModelFactory` to load model from YAML config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration reloaded from /home/jovyan/nfs_share/tod/information_extractor_standalone/config\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Loading /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mLoading \u001b[0m\u001b[1;36m/home/jovyan/nfs_share/models/\u001b[0m\u001b[1;36mLlama-3.2-11B-Vision...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Max tokens: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Max tokens: \u001b[0m\u001b[1;36m2048\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Temperature: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Temperature: \u001b[0m\u001b[1;36m0.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Quantization: </span><span style=\"color: #008080; text-decoration-color: #008080; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Quantization: \u001b[0m\u001b[3;36mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">ğŸš€ Loading Llama Vision model with robust multi-GPU optimization...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mğŸš€ Loading Llama Vision model with robust multi-GPU optimization\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Features: Smart quantization, memory management, V100 support</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFeatures: Smart quantization, memory management, V100 support\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”§ Configuring CUDA memory for Llama...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”§ Configuring CUDA memory for Llama\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ CUDA memory allocation configured: max_split_size_mb:64\n",
      "ğŸ’¡ Using 64MB memory blocks to reduce fragmentation\n",
      "ğŸ“Š Initial CUDA state (Multi-GPU Total): Allocated=0.00GB, Reserved=0.00GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ” Performing robust GPU memory detection...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ” Performing robust GPU memory detection\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting robust GPU memory detection...\n",
      "ğŸ“Š Detected 2 GPU(s), analyzing each device...\n",
      "   GPU 0 (NVIDIA H200): 139.7GB total, 139.7GB available\n",
      "   GPU 1 (NVIDIA H200): 139.7GB total, 139.7GB available\n",
      "\n",
      "======================================================================\n",
      "ğŸ” ROBUST GPU MEMORY DETECTION REPORT\n",
      "======================================================================\n",
      "âœ… Success: 2/2 GPUs detected\n",
      "ğŸ“Š Total Memory: 279.44GB\n",
      "ğŸ’¾ Available Memory: 279.44GB\n",
      "âš¡ Allocated Memory: 0.00GB\n",
      "ğŸ”„ Reserved Memory: 0.00GB\n",
      "ğŸ“¦ Fragmentation: 0.00GB\n",
      "ğŸ–¥ï¸  Multi-GPU: Yes\n",
      "âš–ï¸  Balanced Distribution: Yes\n",
      "\n",
      "ğŸ“‹ Per-GPU Breakdown:\n",
      "   GPU 0 (NVIDIA H200): 139.7GB total, 139.7GB available (0.0% used)\n",
      "   GPU 1 (NVIDIA H200): 139.7GB total, 139.7GB available (0.0% used)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ“Š GPU Hardware: NVIDIA H200 </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">2x 140GB = 279GB total</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ“Š GPU Hardware: NVIDIA H200 \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34m2x 140GB = 279GB total\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ—ï¸ Architecture: datacenter_high_memory </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">dynamic detection</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ—ï¸ Architecture: datacenter_high_memory \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mdynamic detection\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ¯ Model: Llama-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3.2</span><span style=\"color: #000080; text-decoration-color: #000080\">-11B-Vision </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">estimated need: 22GB + </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">6.</span><span style=\"color: #000080; text-decoration-color: #000080\">0GB buffer</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ¯ Model: Llama-\u001b[0m\u001b[1;34m3.2\u001b[0m\u001b[34m-11B-Vision \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mestimated need: 22GB + \u001b[0m\u001b[1;34m6.\u001b[0m\u001b[34m0GB buffer\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ’¾ Available Memory: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">279.</span><span style=\"color: #000080; text-decoration-color: #000080\">4GB across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">GPU(</span><span style=\"color: #000080; text-decoration-color: #000080\">s</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ’¾ Available Memory: \u001b[0m\u001b[1;34m279.\u001b[0m\u001b[34m4GB across \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mGPU\u001b[0m\u001b[1;34m(\u001b[0m\u001b[34ms\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ’¡ Memory sufficient: âœ… Yes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ’¡ Memory sufficient: âœ… Yes\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… datacenter_high_memory with 279GB - running in full precision as requested</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… datacenter_high_memory with 279GB - running in full precision as requested\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ“Š FINAL QUANTIZATION DECISION: DISABLED (full precision)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mğŸ“Š FINAL QUANTIZATION DECISION: DISABLED \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mfull precision\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Total GPU Memory: 279GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Total GPU Memory: 279GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Available Memory: 279GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Available Memory: 279GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Model needs: ~22GB + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>0GB buffer for Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-11B-Vision\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Model needs: ~22GB + \u001b[1;36m6.\u001b[0m0GB buffer for Llama-\u001b[1;36m3.2\u001b[0m-11B-Vision\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Working GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Working GPUs: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ğŸš€ Using </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">16</span><span style=\"color: #008000; text-decoration-color: #008000\">-bit precision for optimal performance</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mğŸš€ Using \u001b[0m\u001b[1;32m16\u001b[0m\u001b[32m-bit precision for optimal performance\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading Llama Vision model...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mLoading Llama Vision model\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”„ Auto-distributing model across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> GPUs...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”„ Auto-distributing model across \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m GPUs\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8757cb519354a0792bbeedfe547ba85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading processor...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mLoading processor\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model and processor loaded successfully!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model and processor loaded successfully!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”„ Multi-GPU Distribution Analysis </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> GPUs</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span><span style=\"color: #000080; text-decoration-color: #000080\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”„ Multi-GPU Distribution Analysis \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m GPUs\u001b[0m\u001b[1;34m)\u001b[0m\u001b[34m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"font-weight: bold\">(</span>NVIDIA H200<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.</span>7GB/150GB <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.6</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   GPU \u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mNVIDIA H200\u001b[1m)\u001b[0m: \u001b[1;36m9.\u001b[0m7GB/150GB \u001b[1m(\u001b[0m\u001b[1;36m6.6\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>NVIDIA H200<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.</span>6GB/150GB <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.8</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   GPU \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0mNVIDIA H200\u001b[1m)\u001b[0m: \u001b[1;36m11.\u001b[0m6GB/150GB \u001b[1m(\u001b[0m\u001b[1;36m7.8\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ“Š Total across all GPUs: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">21.</span><span style=\"color: #000080; text-decoration-color: #000080\">3GB allocated, </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">21.</span><span style=\"color: #000080; text-decoration-color: #000080\">6GB reserved, 300GB capacity</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ“Š Total across all GPUs: \u001b[0m\u001b[1;34m21.\u001b[0m\u001b[34m3GB allocated, \u001b[0m\u001b[1;34m21.\u001b[0m\u001b[34m6GB reserved, 300GB capacity\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model successfully distributed across GPUs</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model successfully distributed across GPUs\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> modules\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;36m0\u001b[0m: \u001b[1;36m18\u001b[0m modules\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> modules\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;36m1\u001b[0m: \u001b[1;36m28\u001b[0m modules\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                        ğŸ”§ Llama Vision Model Configuration                         </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Setting             </span>â”ƒ<span style=\"font-weight: bold\"> Value                  </span>â”ƒ<span style=\"font-weight: bold\"> Llama Status                      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Path          </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> Llama-3.2-11B-Vision   </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Valid                          </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Device Placement    </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> cuda:0                 </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Loaded                         </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Quantization Method </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 16-bit                 </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… 16-bit (Performance Optimized) </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Data Type           </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> bfloat16               </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Recommended                    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Max New Tokens      </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 4000                   </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Generation Ready               </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> GPU Configuration   </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 2x NVIDIA H200 (300GB) </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… 300GB Total                    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Parameters    </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 10,642,941,475         </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Loaded                         </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Memory Optimization </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> Llama Robust           </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… V100 Compatible                </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                        ğŸ”§ Llama Vision Model Configuration                         \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mSetting            \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mValue                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mLlama Status                     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Path         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mLlama-3.2-11B-Vision  \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Valid                         \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mDevice Placement   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mcuda:0                \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Loaded                        \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mQuantization Method\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m16-bit                \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… 16-bit (Performance Optimized)\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mData Type          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mbfloat16              \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Recommended                   \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mMax New Tokens     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m4000                  \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Generation Ready              \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mGPU Configuration  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m2x NVIDIA H200 (300GB)\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… 300GB Total                   \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Parameters   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m10,642,941,475        \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Loaded                        \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mMemory Optimization\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mLlama Robust          \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… V100 Compatible               \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Running model compatibility test...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mRunning model compatibility test\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model compatibility test passed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model compatibility test passed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Performing initial memory cleanup...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mPerforming initial memory cleanup\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ§¹ Memory cleanup completed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ğŸ§¹ Memory cleanup completed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ’¾ Final state <span style=\"font-weight: bold\">(</span>Multi-GPU Total<span style=\"font-weight: bold\">)</span>: <span style=\"color: #808000; text-decoration-color: #808000\">Allocated</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.</span>29GB, <span style=\"color: #808000; text-decoration-color: #808000\">Reserved</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.</span>60GB, <span style=\"color: #808000; text-decoration-color: #808000\">Fragmentation</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>31GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ğŸ’¾ Final state \u001b[1m(\u001b[0mMulti-GPU Total\u001b[1m)\u001b[0m: \u001b[33mAllocated\u001b[0m=\u001b[1;36m21\u001b[0m\u001b[1;36m.\u001b[0m29GB, \u001b[33mReserved\u001b[0m=\u001b[1;36m21\u001b[0m\u001b[1;36m.\u001b[0m60GB, \u001b[33mFragmentation\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.\u001b[0m31GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ğŸ‰ Llama Vision model loading and validation complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mğŸ‰ Llama Vision model loading and validation complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”§ Llama optimizations active: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">16</span><span style=\"color: #000080; text-decoration-color: #000080\">-bit precision, memory management, vision preservation</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”§ Llama optimizations active: \u001b[0m\u001b[1;34m16\u001b[0m\u001b[34m-bit precision, memory management, vision preservation\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Initialized VisionLanguageModel: /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision\n",
      "   Model type: Llama (chat template)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… Vision-language model loaded with LangChain interface</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ… Vision-language model loaded with LangChain interface\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Type: vision_language_model</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mType: vision_language_model\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Model: /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mModel: \u001b[0m\u001b[36m/home/jovyan/nfs_share/models/\u001b[0m\u001b[36mLlama-3.2-11B-Vision\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load YAML configuration\n",
    "yaml_config = get_yaml_config()\n",
    "model_config = yaml_config.get_model_config(CONFIG['MODEL_NAME'])\n",
    "\n",
    "rprint(f\"[bold cyan]Loading {model_config.model_id}...[/bold cyan]\")\n",
    "rprint(f\"[cyan]  Max tokens: {model_config.max_new_tokens}[/cyan]\")\n",
    "rprint(f\"[cyan]  Temperature: {model_config.temperature}[/cyan]\")\n",
    "rprint(f\"[cyan]  Quantization: {model_config.use_quantization}[/cyan]\")\n",
    "\n",
    "# Select appropriate model loader based on model name\n",
    "if 'llama' in CONFIG['MODEL_NAME'].lower():\n",
    "    model_loader = load_llama_model\n",
    "elif 'internvl' in CONFIG['MODEL_NAME'].lower():\n",
    "    model_loader = load_internvl3_model\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {CONFIG['MODEL_NAME']}\")\n",
    "\n",
    "# Create LangChain wrapper with YAML config\n",
    "llm = VisionLanguageModelFactory.from_yaml_config(\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    model_loader_func=model_loader,\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]âœ… Vision-language model loaded with LangChain interface[/bold green]\")\n",
    "rprint(f\"[cyan]Type: {llm._llm_type}[/cyan]\")\n",
    "rprint(f\"[cyan]Model: {llm.model_id}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Processing Pipeline\n",
    "\n",
    "**NEW**: Uses LangChain `DocumentProcessingPipeline` with YAML prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… Processing pipeline created</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ… Processing pipeline created\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Features:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFeatures:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  âœ“ Document type detection\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  âœ“ Document type detection\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  âœ“ Field extraction with Pydantic validation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  âœ“ Field extraction with Pydantic validation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  âœ“ Cleaning disabled - will be done explicitly in separate cell\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  âœ“ Cleaning disabled - will be done explicitly in separate cell\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  âœ“ Hot-reload prompts: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">prompt_manager.reload_config</span><span style=\"font-weight: bold\">()</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  âœ“ Hot-reload prompts: \u001b[1;35mprompt_manager.reload_config\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create processing pipeline\n",
    "pipeline = create_pipeline(\n",
    "    llm=llm,\n",
    "    enable_fixing=CONFIG['ENABLE_FIXING'],\n",
    "    enable_cleaning=False,  # Disable automatic cleaning - we'll do it explicitly in a separate cell\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Create prompt manager for hot-reload capability\n",
    "prompt_manager = LangChainPromptManager(\n",
    "    use_yaml_config=True,\n",
    "    system_mode=CONFIG['SYSTEM_MODE']\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]âœ… Processing pipeline created[/bold green]\")\n",
    "rprint(\"[cyan]Features:[/cyan]\")\n",
    "rprint(\"  âœ“ Document type detection\")\n",
    "rprint(\"  âœ“ Field extraction with Pydantic validation\")\n",
    "rprint(\"  âœ“ Cleaning disabled - will be done explicitly in separate cell\")\n",
    "rprint(\"  âœ“ Hot-reload prompts: prompt_manager.reload_config()\")\n",
    "\n",
    "if CONFIG['ENABLE_FIXING']:\n",
    "    rprint(\"  âœ“ Self-healing parser enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image Discovery and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ”§ Preprocessing </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #008080; text-decoration-color: #008080\"> images </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">adaptive</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ”§ Preprocessing \u001b[0m\u001b[1;36m9\u001b[0m\u001b[36m images \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36madaptive\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Preprocessing complete</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Preprocessing complete\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Ground truth CSV loaded with 9 rows and 20 columns\n",
      "ğŸ“‹ Available columns: ['image_file', 'DOCUMENT_TYPE', 'BUSINESS_ABN', 'BUSINESS_ADDRESS', 'GST_AMOUNT', 'INVOICE_DATE', 'IS_GST_INCLUDED', 'LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'PAYER_ADDRESS', 'PAYER_NAME', 'STATEMENT_DATE_RANGE', 'SUPPLIER_NAME', 'TOTAL_AMOUNT', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
      "âœ… Using 'image_file' as image identifier column\n",
      "âœ… Ground truth mapping created for 9 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Ground truth loaded for </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000\"> images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Ground truth loaded for \u001b[0m\u001b[1;32m9\u001b[0m\u001b[32m images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Ready to process </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mReady to process \u001b[0m\u001b[1;32m9\u001b[0m\u001b[1;32m images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. image_001.png\n",
      "  2. image_002.png\n",
      "  3. image_003.png\n",
      "  4. image_004.png\n",
      "  5. image_005.png\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Discover images\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Image preprocessing (optional)\n",
    "if CONFIG['ENABLE_PREPROCESSING']:\n",
    "    import tempfile\n",
    "    from common.image_preprocessing import (\n",
    "        enhance_statement_quality,\n",
    "        enhance_for_llama,\n",
    "        preprocess_statement_for_llama,\n",
    "        adaptive_enhance,\n",
    "        preprocess_recommended\n",
    "    )\n",
    "    \n",
    "    preprocess_functions = {\n",
    "        'light': enhance_statement_quality,\n",
    "        'moderate': enhance_for_llama,\n",
    "        'aggressive': preprocess_statement_for_llama,\n",
    "        'adaptive': adaptive_enhance,\n",
    "        'recommended': preprocess_recommended\n",
    "    }\n",
    "    \n",
    "    preprocess_fn = preprocess_functions[CONFIG['PREPROCESSING_MODE']]\n",
    "    preprocessed_images = []\n",
    "    preprocessed_dir = Path(tempfile.mkdtemp(prefix='preprocessed_'))\n",
    "    \n",
    "    rprint(f\"[cyan]ğŸ”§ Preprocessing {len(all_images)} images ({CONFIG['PREPROCESSING_MODE']})[/cyan]\")\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        try:\n",
    "            preprocessed_img = preprocess_fn(img_path)\n",
    "            preprocessed_path = preprocessed_dir / Path(img_path).name\n",
    "            preprocessed_img.save(preprocessed_path)\n",
    "            preprocessed_images.append(str(preprocessed_path))\n",
    "        except Exception as e:\n",
    "            rprint(f\"[yellow]âš ï¸ Preprocessing failed for {Path(img_path).name}: {e}[/yellow]\")\n",
    "            preprocessed_images.append(img_path)\n",
    "    \n",
    "    all_images = preprocessed_images\n",
    "    rprint(f\"[green]âœ… Preprocessing complete[/green]\")\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    rprint(f\"[green]âœ… Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "\n",
    "# Apply filters\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing with LangChain Pipeline\n",
    "\n",
    "**NEW**: Uses `pipeline.process_batch()` with LangChain interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸš€ Starting batch processing...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mğŸš€ Starting batch processing\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing batch of 9 images\n",
      "\n",
      "[1/9] \n",
      "============================================================\n",
      "Processing: image_001.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_001.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_001.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[2/9] \n",
      "============================================================\n",
      "Processing: image_002.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_002.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_002.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[3/9] \n",
      "============================================================\n",
      "Processing: image_003.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_003.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_003.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[4/9] \n",
      "============================================================\n",
      "Processing: image_004.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_004.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_004.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[5/9] \n",
      "============================================================\n",
      "Processing: image_005.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_005.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_005.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[6/9] \n",
      "============================================================\n",
      "Processing: image_006.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_006.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_006.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[7/9] \n",
      "============================================================\n",
      "Processing: image_007.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_007.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_007.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[8/9] \n",
      "============================================================\n",
      "Processing: image_008.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_008.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_008.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "[9/9] \n",
      "============================================================\n",
      "Processing: image_009.png\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: Document Type Detection\n",
      "ğŸ” Detecting document type: image_009.png\n",
      "\n",
      "âŒ Processing failed: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "============================================================\n",
      "\n",
      "âŒ Error processing image_009.png: No chat template is set for this processor. Please either set the `chat_template` attribute, or provide a chat template as an argument. See https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\n",
      "\n",
      "âœ… Batch complete: 0/9 successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… Processed </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> images in </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ… Processed \u001b[0m\u001b[1;32m9\u001b[0m\u001b[1;32m images in \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[1;32m5s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Average time: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span><span style=\"color: #008080; text-decoration-color: #008080\">05s per image</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAverage time: \u001b[0m\u001b[1;36m0.\u001b[0m\u001b[36m05s per image\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Document types found: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080\">'error'</span><span style=\"color: #008080; text-decoration-color: #008080\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mDocument types found: \u001b[0m\u001b[1;36m{\u001b[0m\u001b[36m'error'\u001b[0m\u001b[36m: \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;36m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Success rate: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"color: #008080; text-decoration-color: #008080\">%</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mSuccess rate: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m9\u001b[0m\u001b[36m \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[36m%\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">âš ï¸  </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">9</span><span style=\"color: #808000; text-decoration-color: #808000\"> documents had processing errors</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mâš ï¸  \u001b[0m\u001b[1;33m9\u001b[0m\u001b[33m documents had processing errors\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process batch using LangChain pipeline\n",
    "rprint(\"[bold cyan]ğŸš€ Starting batch processing...[/bold cyan]\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "batch_results = pipeline.process_batch(all_images)\n",
    "end_time = datetime.now()\n",
    "\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "processing_times = [total_time / len(batch_results)] * len(batch_results)  # Estimate individual times\n",
    "\n",
    "# Extract document types\n",
    "document_types_found = {}\n",
    "for result in batch_results:\n",
    "    doc_type = result.get('document_type', 'unknown')\n",
    "    document_types_found[doc_type] = document_types_found.get(doc_type, 0) + 1\n",
    "\n",
    "# Summary\n",
    "rprint(f\"[bold green]âœ… Processed {len(batch_results)} images in {total_time:.1f}s[/bold green]\")\n",
    "rprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s per image[/cyan]\")\n",
    "rprint(f\"[cyan]Document types found: {document_types_found}[/cyan]\")\n",
    "\n",
    "# Count successful vs failed\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "failed = len(batch_results) - successful\n",
    "rprint(f\"[cyan]Success rate: {successful}/{len(batch_results)} ({successful/len(batch_results)*100:.1f}%)[/cyan]\")\n",
    "if failed > 0:\n",
    "    rprint(f\"[yellow]âš ï¸  {failed} documents had processing errors[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Clean and Normalize Extracted Data\n",
    "\n",
    "**CRITICAL**: Apply ExtractionCleaner to normalize field values for accuracy.\n",
    "\n",
    "This step transforms raw model output into clean, ground-truth-compatible format:\n",
    "- **ABN**: `06082698025` â†’ `06 082 698 025`\n",
    "- **Monetary**: `$4,834.03` â†’ `$4834.03` (remove commas)\n",
    "- **Addresses**: Remove phone/email, normalize spacing\n",
    "- **Dates**: `Thu 04 Sep 2025` â†’ `04/09/2025`\n",
    "- **Document types**: `STATEMENT` â†’ `BANK_STATEMENT`\n",
    "- **Lists**: Convert to pipe-separated format\n",
    "\n",
    "**Expected impact**: Restores accuracy from ~49% â†’ ~82% (81.8% baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ§¹ Cleaning and normalizing extracted data...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mğŸ§¹ Cleaning and normalizing extracted data\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Cleaned </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span><span style=\"color: #008000; text-decoration-color: #008000\"> documents</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Cleaned \u001b[0m\u001b[1;32m0\u001b[0m\u001b[32m documents\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Total field transformations: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mTotal field transformations: \u001b[0m\u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    111\u001b[39m rprint(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[green]âœ… Cleaned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaning_stats[\u001b[33m'\u001b[39m\u001b[33mdocuments_cleaned\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents[/green]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m rprint(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[cyan]Total field transformations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaning_stats[\u001b[33m'\u001b[39m\u001b[33mtotal_fields_cleaned\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m[/cyan]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m rprint(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[cyan]Average per document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcleaning_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_fields_cleaned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m/\u001b[49m\u001b[43mcleaning_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdocuments_cleaned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fields[/cyan]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Show examples\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cleaning_stats[\u001b[33m'\u001b[39m\u001b[33mcleaning_examples\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "from common.extraction_cleaner import ExtractionCleaner\n",
    "\n",
    "\n",
    "def serialize_pydantic_to_llm_format(field_name: str, value) -> str:\n",
    "    \"\"\"\n",
    "    Convert Pydantic model types to LLM output format strings.\n",
    "    \n",
    "    This prepares types for ExtractionCleaner which expects LLM string output.\n",
    "    \"\"\"\n",
    "    # Handle NOT_FOUND\n",
    "    if value == \"NOT_FOUND\" or value is None:\n",
    "        return \"NOT_FOUND\"\n",
    "    \n",
    "    # Handle lists - convert to comma-separated (cleaner will convert to pipes)\n",
    "    if isinstance(value, list):\n",
    "        if not value:\n",
    "            return \"NOT_FOUND\"\n",
    "        # Convert each item to string\n",
    "        str_items = []\n",
    "        for item in value:\n",
    "            if isinstance(item, Decimal):\n",
    "                str_items.append(f\"${str(item)}\")\n",
    "            else:\n",
    "                str_items.append(str(item))\n",
    "        return \", \".join(str_items)\n",
    "    \n",
    "    # Handle Decimal - convert to string with $ for monetary fields\n",
    "    if isinstance(value, Decimal):\n",
    "        return f\"${str(value)}\"\n",
    "    \n",
    "    # Handle boolean - convert to lowercase string\n",
    "    if isinstance(value, bool):\n",
    "        return str(value).lower()\n",
    "    \n",
    "    # Everything else - just convert to string\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "# Create cleaner instance\n",
    "cleaner = ExtractionCleaner(debug=False)  # Set to True to see each field transformation\n",
    "\n",
    "rprint(\"[bold cyan]ğŸ§¹ Cleaning and normalizing extracted data...[/bold cyan]\")\n",
    "\n",
    "# Track cleaning statistics\n",
    "cleaning_stats = {\n",
    "    'documents_cleaned': 0,\n",
    "    'total_fields_cleaned': 0,\n",
    "    'cleaning_examples': []\n",
    "}\n",
    "\n",
    "# Clean each document's extracted data\n",
    "for i, result in enumerate(batch_results):\n",
    "    if 'extracted_data' not in result:\n",
    "        continue\n",
    "    \n",
    "    extracted_data = result['extracted_data']\n",
    "    \n",
    "    # Convert Pydantic model to dict\n",
    "    if hasattr(extracted_data, 'model_dump'):\n",
    "        raw_dict = extracted_data.model_dump()\n",
    "    elif hasattr(extracted_data, 'dict'):\n",
    "        raw_dict = extracted_data.dict()\n",
    "    else:\n",
    "        continue  # Skip if not a Pydantic model\n",
    "    \n",
    "    # CRITICAL: Serialize Python types to LLM format strings\n",
    "    # This converts Listsâ†’strings, Decimalsâ†’\"$123.45\", boolsâ†’\"true\"/\"false\"\n",
    "    serialized_dict = {\n",
    "        field: serialize_pydantic_to_llm_format(field, value)\n",
    "        for field, value in raw_dict.items()\n",
    "    }\n",
    "    \n",
    "    # Clean all fields (now expects LLM string output)\n",
    "    cleaned_dict = cleaner.clean_extraction_dict(serialized_dict)\n",
    "    \n",
    "    # Count changes\n",
    "    changes_made = sum(1 for field in serialized_dict if str(serialized_dict[field]) != str(cleaned_dict[field]))\n",
    "    \n",
    "    # Store first few examples for display\n",
    "    if changes_made > 0 and len(cleaning_stats['cleaning_examples']) < 3:\n",
    "        doc_example = {\n",
    "            'image_name': result.get('image_name', 'unknown'),\n",
    "            'document_type': result.get('document_type', 'unknown'),\n",
    "            'fields_changed': changes_made,\n",
    "            'examples': []\n",
    "        }\n",
    "        \n",
    "        for field in serialized_dict:\n",
    "            raw_value = str(serialized_dict[field])\n",
    "            cleaned_value = str(cleaned_dict[field])\n",
    "            if raw_value != cleaned_value and len(doc_example['examples']) < 3:\n",
    "                doc_example['examples'].append({\n",
    "                    'field': field,\n",
    "                    'raw': raw_value[:80] + '...' if len(raw_value) > 80 else raw_value,\n",
    "                    'cleaned': cleaned_value[:80] + '...' if len(cleaned_value) > 80 else cleaned_value\n",
    "                })\n",
    "        \n",
    "        if doc_example['examples']:\n",
    "            cleaning_stats['cleaning_examples'].append(doc_example)\n",
    "    \n",
    "    cleaning_stats['total_fields_cleaned'] += changes_made\n",
    "    cleaning_stats['documents_cleaned'] += 1\n",
    "    \n",
    "    # Store cleaned dict directly (preserves pipe-separated strings)\n",
    "    result['extracted_data'] = cleaned_dict\n",
    "    result['cleaning_applied'] = True\n",
    "\n",
    "# Display cleaning summary\n",
    "rprint(f\"[green]âœ… Cleaned {cleaning_stats['documents_cleaned']} documents[/green]\")\n",
    "rprint(f\"[cyan]Total field transformations: {cleaning_stats['total_fields_cleaned']}[/cyan]\")\n",
    "rprint(f\"[cyan]Average per document: {cleaning_stats['total_fields_cleaned']/cleaning_stats['documents_cleaned']:.1f} fields[/cyan]\")\n",
    "\n",
    "# Show examples\n",
    "if cleaning_stats['cleaning_examples']:\n",
    "    rprint(\"\\n[bold yellow]ğŸ“‹ Cleaning Examples:[/bold yellow]\")\n",
    "    for example in cleaning_stats['cleaning_examples']:\n",
    "        rprint(f\"\\n[yellow]{example['image_name']} ({example['document_type']}) - {example['fields_changed']} fields changed:[/yellow]\")\n",
    "        for ex in example['examples']:\n",
    "            rprint(f\"  [cyan]{ex['field']}:[/cyan]\")\n",
    "            rprint(f\"    Raw:     {ex['raw']}\")\n",
    "            rprint(f\"    Cleaned: {ex['cleaned']}\")\n",
    "else:\n",
    "    rprint(\"\\n[dim]No field transformations needed - all values already clean[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Evaluation Against Ground Truth\n",
    "\n",
    "**NEW**: Calculate accuracy metrics by comparing extracted data with ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for each result (if in evaluation mode)\n",
    "if not CONFIG['INFERENCE_ONLY'] and ground_truth:\n",
    "    from common.config import get_document_type_fields\n",
    "    from common.evaluation_metrics import calculate_field_accuracy_with_method\n",
    "    \n",
    "    rprint(\"[cyan]ğŸ” Calculating evaluation metrics...[/cyan]\")\n",
    "    \n",
    "    # DEBUG: Track what fields are being used per document type\n",
    "    fields_per_doc_type = {}\n",
    "    \n",
    "    # DEBUG: Show first document comparison\n",
    "    debug_shown = False\n",
    "    \n",
    "    for result in batch_results:\n",
    "        img_name = result['image_name']\n",
    "        \n",
    "        if img_name not in ground_truth:\n",
    "            result['evaluation'] = {'inference_only': True}\n",
    "            continue\n",
    "        \n",
    "        # Get extracted data as dict\n",
    "        extracted_data = result.get('extracted_data', {})\n",
    "        if hasattr(extracted_data, 'model_dump'):\n",
    "            extracted_dict = extracted_data.model_dump()\n",
    "        elif hasattr(extracted_data, 'dict'):\n",
    "            extracted_dict = extracted_data.dict()\n",
    "        else:\n",
    "            extracted_dict = extracted_data if isinstance(extracted_data, dict) else {}\n",
    "        \n",
    "        # Get ground truth for this image\n",
    "        truth_data = ground_truth[img_name]\n",
    "        doc_type = result.get('document_type', '')\n",
    "        \n",
    "        # Get fields for this document type\n",
    "        doc_fields = get_document_type_fields(doc_type)\n",
    "        \n",
    "        # DEBUG: Show detailed comparison for first document\n",
    "        if not debug_shown and doc_type == 'receipt':\n",
    "            rprint(f\"\\n[bold yellow]ğŸ” DEBUG: Comparing {img_name} ({doc_type}):[/bold yellow]\")\n",
    "            rprint(f\"[yellow]Fields to evaluate: {doc_fields}[/yellow]\")\n",
    "            for field in doc_fields[:3]:  # Show first 3 fields\n",
    "                ext_val = extracted_dict.get(field, 'NOT_FOUND')\n",
    "                truth_val = truth_data.get(field, 'NOT_FOUND')\n",
    "                rprint(f\"\\n[cyan]{field}:[/cyan]\")\n",
    "                rprint(f\"  Extracted: {ext_val!r} (type: {type(ext_val).__name__})\")\n",
    "                rprint(f\"  Truth:     {truth_val!r} (type: {type(truth_val).__name__})\")\n",
    "                rprint(f\"  Match: {ext_val == truth_val}\")\n",
    "            debug_shown = True\n",
    "        \n",
    "        # DEBUG: Track fields used per document type\n",
    "        if doc_type not in fields_per_doc_type:\n",
    "            fields_per_doc_type[doc_type] = {\n",
    "                'field_count': len(doc_fields),\n",
    "                'fields': doc_fields,\n",
    "                'sample_count': 0\n",
    "            }\n",
    "        fields_per_doc_type[doc_type]['sample_count'] += 1\n",
    "        \n",
    "        # Calculate field-level accuracies\n",
    "        field_accuracies = {}\n",
    "        total_fields = 0\n",
    "        fields_matched = 0\n",
    "        fields_extracted = 0\n",
    "        \n",
    "        for field in doc_fields:\n",
    "            total_fields += 1\n",
    "            extracted_value = extracted_dict.get(field, 'NOT_FOUND')\n",
    "            truth_value = truth_data.get(field, 'NOT_FOUND')\n",
    "            \n",
    "            # Count extracted fields\n",
    "            if extracted_value != 'NOT_FOUND':\n",
    "                fields_extracted += 1\n",
    "            \n",
    "            # Calculate accuracy for this field (returns dict with f1_score, precision, recall, etc.)\n",
    "            accuracy_result = calculate_field_accuracy_with_method(\n",
    "                extracted_value,\n",
    "                truth_value,\n",
    "                field,\n",
    "                method='order_aware_f1',  # Use configured method\n",
    "                debug=False\n",
    "            )\n",
    "            \n",
    "            # Extract f1_score from the result dict\n",
    "            accuracy_score = accuracy_result.get('f1_score', 0.0) if isinstance(accuracy_result, dict) else float(accuracy_result)\n",
    "            \n",
    "            field_accuracies[field] = {\n",
    "                'accuracy': accuracy_score,\n",
    "                'extracted': str(extracted_value),\n",
    "                'ground_truth': str(truth_value)\n",
    "            }\n",
    "            \n",
    "            # Count matched fields (accuracy > 0)\n",
    "            if accuracy_score > 0:\n",
    "                fields_matched += 1\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        overall_accuracy = sum(fa['accuracy'] for fa in field_accuracies.values()) / total_fields if total_fields > 0 else 0\n",
    "        \n",
    "        # Store evaluation results\n",
    "        result['evaluation'] = {\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'fields_extracted': fields_extracted,\n",
    "            'fields_matched': fields_matched,\n",
    "            'total_fields': total_fields,\n",
    "            'field_accuracies': field_accuracies,\n",
    "            'inference_only': False\n",
    "        }\n",
    "    \n",
    "    rprint(\"[green]âœ… Evaluation metrics calculated[/green]\")\n",
    "    \n",
    "    # DEBUG: Show what fields were used per document type\n",
    "    rprint(\"\\n[bold yellow]ğŸ” Debug: Fields used per document type:[/bold yellow]\")\n",
    "    for doc_type, info in fields_per_doc_type.items():\n",
    "        rprint(f\"[yellow]  {doc_type}: {info['field_count']} fields ({info['sample_count']} documents)[/yellow]\")\n",
    "        rprint(f\"[dim]    Fields: {', '.join(info['fields'][:5])}{'...' if len(info['fields']) > 5 else ''}[/dim]\")\n",
    "    \n",
    "    # Show average accuracy\n",
    "    evaluated = [r for r in batch_results if not r.get('evaluation', {}).get('inference_only', True)]\n",
    "    if evaluated:\n",
    "        avg_accuracy = np.mean([r['evaluation']['overall_accuracy'] * 100 for r in evaluated])\n",
    "        rprint(f\"\\n[bold cyan]ğŸ“Š Average Accuracy: {avg_accuracy:.1f}%[/bold cyan]\")\n",
    "else:\n",
    "    rprint(\"[yellow]âš ï¸ Skipping evaluation - inference-only mode or no ground truth[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Analytics and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field columns dynamically from config (excluding validation-only fields)\n",
    "from common.config import get_extraction_fields, get_validation_only_fields\n",
    "\n",
    "# Get all extraction fields\n",
    "all_fields = get_extraction_fields()\n",
    "\n",
    "# Get validation-only fields to exclude\n",
    "validation_only = get_validation_only_fields()\n",
    "\n",
    "# Filter out validation-only fields\n",
    "FIELD_COLUMNS = [f for f in all_fields if f not in validation_only]\n",
    "\n",
    "rprint(f\"[cyan]ğŸ“‹ Using {len(FIELD_COLUMNS)} evaluation fields (excluded {len(validation_only)} validation-only)[/cyan]\")\n",
    "if validation_only:\n",
    "    rprint(f\"[dim]Excluded: {', '.join(validation_only)}[/dim]\")\n",
    "\n",
    "# Add processing_time and prompt_used to batch_results for BatchAnalytics\n",
    "for i, result in enumerate(batch_results):\n",
    "    result['processing_time'] = processing_times[i]\n",
    "    result['prompt_used'] = f\"{CONFIG['SYSTEM_MODE']}_mode\"\n",
    "    # Add evaluation stub if not present (for inference-only mode)\n",
    "    if 'evaluation' not in result:\n",
    "        result['evaluation'] = {'inference_only': True}\n",
    "\n",
    "# Create CSV data\n",
    "csv_data = []\n",
    "for i, result in enumerate(batch_results):\n",
    "    image_name = Path(result['image_path']).name\n",
    "    doc_type = result.get('document_type', '').lower()\n",
    "    extracted_data = result.get('extracted_data', {})\n",
    "    \n",
    "    # Convert Pydantic model to dict if needed\n",
    "    if hasattr(extracted_data, 'model_dump'):\n",
    "        extracted_fields = extracted_data.model_dump()\n",
    "    elif hasattr(extracted_data, 'dict'):\n",
    "        extracted_fields = extracted_data.dict()\n",
    "    else:\n",
    "        extracted_fields = extracted_data if isinstance(extracted_data, dict) else {}\n",
    "    \n",
    "    # Count fields\n",
    "    found_fields = sum(1 for field in FIELD_COLUMNS \n",
    "                      if extracted_fields.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n",
    "    field_coverage = (found_fields / len(FIELD_COLUMNS) * 100)\n",
    "    \n",
    "    # Evaluation data\n",
    "    evaluation = result.get('evaluation', {})\n",
    "    if CONFIG['INFERENCE_ONLY'] or not evaluation or evaluation.get('inference_only'):\n",
    "        overall_accuracy = None\n",
    "    else:\n",
    "        overall_accuracy = evaluation.get('overall_accuracy', 0) * 100\n",
    "    \n",
    "    row_data = {\n",
    "        'image_file': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_times[i],\n",
    "        'found_fields': found_fields,\n",
    "        'field_coverage': field_coverage,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'model_used': CONFIG['MODEL_NAME'],\n",
    "        'system_mode': CONFIG['SYSTEM_MODE'],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    # Add field values\n",
    "    for field in FIELD_COLUMNS:\n",
    "        row_data[field] = extracted_fields.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    csv_data.append(row_data)\n",
    "\n",
    "# Create and save DataFrame\n",
    "model_short_name = CONFIG['MODEL_NAME'].split('-')[0]  # llama or internvl3\n",
    "results_df = pd.DataFrame(csv_data)\n",
    "csv_path = OUTPUT_DIRS['csv'] / f\"{model_short_name}_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]âœ… Results exported to CSV[/bold green]\")\n",
    "rprint(f\"[cyan]ğŸ“„ File: {csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]ğŸ“Š Structure: {len(results_df)} rows Ã— {len(results_df.columns)} columns[/cyan]\")\n",
    "\n",
    "# Display sample\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', 'field_coverage']\n",
    "else:\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', 'found_fields']\n",
    "\n",
    "rprint(\"\\n[bold blue]ğŸ“‹ Sample Results:[/bold blue]\")\n",
    "display(results_df[sample_cols].head(3))\n",
    "\n",
    "# Create analytics\n",
    "analytics = BatchAnalytics(batch_results, processing_times)\n",
    "saved_files, df_results, df_summary, df_doctype_stats, df_field_stats = analytics.save_all_dataframes(\n",
    "    OUTPUT_DIRS['csv'], BATCH_TIMESTAMP, verbose=False\n",
    ")\n",
    "\n",
    "rprint(\"\\n[bold blue]ğŸ“Š Results Summary[/bold blue]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(f\"[cyan]âœ… Total images: {len(batch_results)}[/cyan]\")\n",
    "    rprint(f\"[cyan]âœ… Avg fields found: {results_df['found_fields'].mean():.1f}[/cyan]\")\n",
    "    rprint(f\"[cyan]âœ… Avg coverage: {results_df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "else:\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = BatchVisualizer()\n",
    "\n",
    "viz_files = visualizer.create_all_visualizations(\n",
    "    df_results,\n",
    "    df_doctype_stats,\n",
    "    OUTPUT_DIRS['visualizations'],\n",
    "    BATCH_TIMESTAMP,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "rprint(f\"[green]âœ… Visualizations saved to {OUTPUT_DIRS['visualizations']}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = BatchReporter(\n",
    "    batch_results,\n",
    "    processing_times,\n",
    "    document_types_found,\n",
    "    BATCH_TIMESTAMP\n",
    ")\n",
    "\n",
    "report_files = reporter.save_all_reports(\n",
    "    OUTPUT_DIRS,\n",
    "    df_results,\n",
    "    df_summary,\n",
    "    df_doctype_stats,\n",
    "    CONFIG['MODEL_NAME'],\n",
    "    {\n",
    "        'data_dir': CONFIG['DATA_DIR'],\n",
    "        'ground_truth': CONFIG['GROUND_TRUTH'],\n",
    "        'max_images': CONFIG['MAX_IMAGES'],\n",
    "    },\n",
    "    {\n",
    "        'model_name': CONFIG['MODEL_NAME'],\n",
    "        'system_mode': CONFIG['SYSTEM_MODE'],\n",
    "        'enable_fixing': CONFIG['ENABLE_FIXING'],\n",
    "        'preprocessing': CONFIG['PREPROCESSING_MODE'] if CONFIG['ENABLE_PREPROCESSING'] else 'disabled'\n",
    "    },\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "rprint(f\"[green]âœ… Reports saved to {OUTPUT_DIRS['reports']}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Final Summary and Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.rule(\"[bold green]Processing Complete[/bold green]\")\n",
    "\n",
    "total_images = len(batch_results)\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "\n",
    "rprint(f\"[bold green]âœ… Processed: {total_images} images[/bold green]\")\n",
    "rprint(f\"[cyan]Success Rate: {(successful/total_images*100):.1f}%[/cyan]\")\n",
    "\n",
    "if not CONFIG['INFERENCE_ONLY']:\n",
    "    avg_accuracy = results_df['overall_accuracy'].mean() if 'overall_accuracy' in results_df else 0\n",
    "    rprint(f\"[cyan]Average Accuracy: {avg_accuracy:.2f}%[/cyan]\")\n",
    "\n",
    "rprint(f\"[cyan]Model: {CONFIG['MODEL_NAME']}[/cyan]\")\n",
    "rprint(f\"[cyan]System Mode: {CONFIG['SYSTEM_MODE']}[/cyan]\")\n",
    "rprint(f\"[cyan]Output: {OUTPUT_BASE}[/cyan]\")\n",
    "\n",
    "# Display dashboard\n",
    "dashboard_files = list(OUTPUT_DIRS['visualizations'].glob(f\"dashboard_{BATCH_TIMESTAMP}.png\"))\n",
    "if dashboard_files:\n",
    "    rprint(\"\\n[bold blue]ğŸ“Š Visual Dashboard:[/bold blue]\")\n",
    "    display(Image(str(dashboard_files[0])))\n",
    "\n",
    "# Hot-reload info\n",
    "rprint(\"\\n[bold cyan]ğŸ’¡ Hot-Reload Tips:[/bold cyan]\")\n",
    "rprint(\"  â€¢ Edit prompts: config/prompts.yaml\")\n",
    "rprint(\"  â€¢ Reload: prompt_manager.reload_config()\")\n",
    "rprint(\"  â€¢ Switch models: Change CONFIG['MODEL_NAME'] and rerun Section 5\")\n",
    "rprint(\"  â€¢ Change system mode: Edit CONFIG['SYSTEM_MODE'] (expert/precise/flexible/strict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Zero Accuracy Analysis (Evaluation Mode Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CONFIG['INFERENCE_ONLY']:\n",
    "    zero_accuracy_count = 0\n",
    "    zero_accuracy_images = []\n",
    "    total_evaluated = 0\n",
    "    \n",
    "    for result in batch_results:\n",
    "        evaluation = result.get('evaluation', {})\n",
    "        if evaluation and not evaluation.get('inference_only', False):\n",
    "            total_evaluated += 1\n",
    "            accuracy = evaluation.get('overall_accuracy', 0)\n",
    "            \n",
    "            if accuracy == 0.0:\n",
    "                zero_accuracy_count += 1\n",
    "                zero_accuracy_images.append({\n",
    "                    'image_name': result.get('image_name', 'unknown'),\n",
    "                    'document_type': result.get('document_type', 'unknown'),\n",
    "                    'fields_extracted': evaluation.get('fields_extracted', 0),\n",
    "                    'total_fields': evaluation.get('total_fields', 0),\n",
    "                })\n",
    "    \n",
    "    console.rule(\"[bold red]Zero Accuracy Analysis[/bold red]\")\n",
    "    rprint(f\"[cyan]Total documents evaluated: {total_evaluated}[/cyan]\")\n",
    "    rprint(f\"[red]Documents with 0% accuracy: {zero_accuracy_count}[/red]\")\n",
    "    \n",
    "    if zero_accuracy_count > 0:\n",
    "        percentage = (zero_accuracy_count / total_evaluated) * 100\n",
    "        rprint(f\"[red]Zero accuracy rate: {percentage:.1f}%[/red]\")\n",
    "        rprint(\"\\n[bold red]Documents with 0% Accuracy:[/bold red]\")\n",
    "        for i, img_info in enumerate(zero_accuracy_images, 1):\n",
    "            rprint(f\"  {i}. {img_info['image_name']} ({img_info['document_type']})\")\n",
    "            rprint(f\"     Fields: {img_info['fields_extracted']}/{img_info['total_fields']}\")\n",
    "    else:\n",
    "        rprint(\"[green]âœ… No documents with 0% accuracy - all extractions had some success![/green]\")\n",
    "else:\n",
    "    rprint(\"[yellow]âš ï¸ Zero accuracy analysis requires evaluation mode[/yellow]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
