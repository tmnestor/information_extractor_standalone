# Model Configurations for Document Extraction
# Edit this file to add new models or change settings

models:
  # Llama-3.2-Vision Models
  llama-3.2-11b-vision:
    model_id: "meta-llama/Llama-3.2-11B-Vision-Instruct"
    device: "cuda"
    device_map: "auto"
    max_new_tokens: 2048
    temperature: 0.0
    do_sample: false
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"
    use_quantization: false
    description: "Llama 3.2 Vision 11B - Full precision, best quality"

  llama-3.2-11b-vision-8bit:
    model_id: "meta-llama/Llama-3.2-11B-Vision-Instruct"
    device: "cuda"
    device_map: "auto"
    max_new_tokens: 2048
    temperature: 0.0
    do_sample: false
    load_in_8bit: true
    load_in_4bit: false
    torch_dtype: "bfloat16"
    use_quantization: true
    description: "Llama 3.2 Vision 11B - 8-bit quantized, memory efficient"

  # InternVL3 Models
  internvl3-2b:
    model_id: "OpenGVLab/InternVL3-2B"
    device: "cuda"
    device_map: "auto"
    max_new_tokens: 2048
    temperature: 0.0
    do_sample: false
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"
    use_quantization: false
    description: "InternVL3 2B - Lightweight, fast inference"

  internvl3-8b:
    model_id: "OpenGVLab/InternVL3-8B"
    device: "cuda"
    device_map: "auto"
    max_new_tokens: 2048
    temperature: 0.0
    do_sample: false
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"
    use_quantization: false
    description: "InternVL3 8B - Strong performance, good balance"

  internvl3-8b-quantized:
    model_id: "OpenGVLab/InternVL3-8B"
    device: "cuda"
    device_map: "auto"
    max_new_tokens: 2048
    temperature: 0.0
    do_sample: false
    load_in_8bit: true
    load_in_4bit: false
    torch_dtype: "bfloat16"
    use_quantization: true
    description: "InternVL3 8B - Quantized for V100 compatibility"

  internvl3_5-8b:
    model_id: "OpenGVLab/InternVL3_5-8B"
    device: "cuda"
    device_map: "auto"
    max_new_tokens: 2048
    temperature: 0.0
    do_sample: false
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"
    use_quantization: false
    description: "InternVL3.5 8B - Latest version, improved performance"

# Default model to use when none specified
default_model: "llama-3.2-11b-vision"

# Model paths configuration
# UPDATE THESE PATHS for your environment
# Download models from:
#   - Llama: https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct
#   - InternVL3: https://huggingface.co/OpenGVLab/InternVL3-2B or InternVL3-8B
model_paths:
  # Llama models
  llama:
    production: "/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct"
    # development: "/nfs_share/models/Llama-3.2-11B-Vision-Instruct"
    # efs: "/efs/shared/PTM/Llama-3.2-11B-Vision-Instruct"
    # local: "~/models/Llama-3.2-11B-Vision-Instruct"

  # InternVL3 models - specific paths for each variant
  internvl3-2b:
    production: "/home/jovyan/nfs_share/models/InternVL3-2B"
    # development: "/nfs_share/models/InternVL3-2B"

  internvl3-8b:
    production: "/home/jovyan/nfs_share/models/InternVL3-8B"
    # development: "/nfs_share/models/InternVL3-8B"

  internvl3-8b-quantized:
    production: "/home/jovyan/nfs_share/models/InternVL3-8B"
    # Uses same 8B model but with quantization enabled

  internvl3_5-8b:
    production: "/home/jovyan/nfs_share/models/InternVL3_5-8B"
    # development: "/nfs_share/models/InternVL3_5-8B"

  # Generic fallback (if specific model path not found)
  internvl3:
    production: "/home/jovyan/nfs_share/models/InternVL3-8B"

# Active deployment environment
active_environment: "production"
